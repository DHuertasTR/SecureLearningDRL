{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "from gym import Env\r\n",
    "from gym.spaces import Discrete, Box\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import pandas as pd\r\n",
    "import tensorflow\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Flatten\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from rl.agents import DQNAgent\r\n",
    "from rl.policy import BoltzmannQPolicy\r\n",
    "from rl.memory import SequentialMemory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "data=pd.read_csv('datasets/malicious_data_generated.csv')\r\n",
    "npdata=data.to_numpy()\r\n",
    "malData=np.copy(npdata)\r\n",
    "print(type(malData[1,:]))\r\n",
    "print(malData[:1].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "class MalwareEnv():\r\n",
    "    def __init__(self):\r\n",
    "        # Actions we can take, decrease, increse, none\r\n",
    "        self.action_space = Discrete(20)\r\n",
    "        # max-min array\r\n",
    "        self.observation_space = Box(np.asarray([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]),np.asarray([100, 100, 100, 100, 100, 100, 100, 100, 100, 100]))\r\n",
    "        # Set start \r\n",
    "        self.state = np.asarray(malData[random.randint(0,499),:])\r\n",
    "        \r\n",
    "        # Set time \r\n",
    "        self.length = 60\r\n",
    "        \r\n",
    "    def step(self, action):\r\n",
    "        # Apply action for each state\r\n",
    "        if(action<10):\r\n",
    "            if(action==0):\r\n",
    "                self.state[0]=self.state[0]+1\r\n",
    "            elif(action==1):\r\n",
    "                 self.state[1]=self.state[1]+1\r\n",
    "            elif(action==2):\r\n",
    "                 self.state[2]=self.state[2]+1\r\n",
    "            elif(action==3):\r\n",
    "                 self.state[3]=self.state[3]+1\r\n",
    "            elif(action==4):\r\n",
    "                 self.state[4]=self.state[4]+1\r\n",
    "            elif(action==5):\r\n",
    "                 self.state[5]=self.state[5]+1\r\n",
    "            elif(action==6):\r\n",
    "                 self.state[6]=self.state[6]+1\r\n",
    "            elif(action==7):\r\n",
    "                 self.state[7]=self.state[7]+1\r\n",
    "            elif(action==8):\r\n",
    "                 self.state[8]=self.state[8]+1\r\n",
    "            else:\r\n",
    "                 self.state[9]=self.state[9]+1                     \r\n",
    "        else:\r\n",
    "            if(action==10):\r\n",
    "                self.state[0]+=self.state[0]-1\r\n",
    "            elif(action==11):\r\n",
    "                 self.state[1]+=self.state[1]-1\r\n",
    "            elif(action==12):\r\n",
    "                 self.state[2]+=self.state[2]-1\r\n",
    "            elif(action==13):\r\n",
    "                 self.state[3]+=self.state[3]-1\r\n",
    "            elif(action==14):\r\n",
    "                 self.state[4]+=self.state[4]-1\r\n",
    "            elif(action==15):\r\n",
    "                 self.state[5]+=self.state[5]-1\r\n",
    "            elif(action==16):\r\n",
    "                 self.state[6]+=self.state[6]-1\r\n",
    "            elif(action==17):\r\n",
    "                 self.state[7]+=self.state[7]-1\r\n",
    "            elif(action==18):\r\n",
    "                 self.state[8]+=self.state[8]-1\r\n",
    "            else:\r\n",
    "                 self.state[9]+=self.state[9]-1  \r\n",
    "        \r\n",
    "        self.length -= 1 \r\n",
    "            \r\n",
    "        \r\n",
    "        # Calculate reward in ranges\r\n",
    "        rewardMulti=0 \r\n",
    "        \r\n",
    "        if(self.state[0]>=-0.290698 and self.state[0]<=-133.441860):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[1]>=0 and self.state[1]<=1184):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[2]>=-0.666667 and self.state[2]<=10.666667):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[3]>=-0.312383 and self.state[3]<=109.259173):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[4]>=0 and self.state[4]<=30):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[5]>=-0.322 and self.state[5]<=127.488889):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[6]>=-0.282353 and self.state[6]<=147.976471):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[7]>=-0.164688 and self.state[7]<=715.616633):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[8]>=-0.324081 and self.state[8]<=106.407677):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "        elif(self.state[9]>=-0.750000 and self.state[9]<=227.5):\r\n",
    "                 rewardMulti=rewardMulti+1\r\n",
    "                 \r\n",
    "        if(rewardMulti==0):\r\n",
    "            reward=-1\r\n",
    "        else:\r\n",
    "            reward=rewardMulti\r\n",
    "                 \r\n",
    "        \r\n",
    "        # Check if is done\r\n",
    "        if self.length <= 0: \r\n",
    "            done = True\r\n",
    "        else:\r\n",
    "            done = False\r\n",
    "        \r\n",
    "        \r\n",
    "        info = {}\r\n",
    "        \r\n",
    "        # Return step information\r\n",
    "        return self.state, reward, done, info\r\n",
    "\r\n",
    "    def render(self):\r\n",
    "        pass\r\n",
    "    \r\n",
    "    def reset(self):\r\n",
    "        # Reset \r\n",
    "        self.state = malData[random.randint(0,498),:]\r\n",
    "        # Reset time\r\n",
    "        self.length = 60 \r\n",
    "        return self.state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "import gym\r\n",
    "import random\r\n",
    "from tensorflow.keras import Sequential\r\n",
    "from collections import deque\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "#load environment\r\n",
    "env = MalwareEnv()\r\n",
    "np.random.seed(0)\r\n",
    "class DQN:\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    def __init__(self, action_space, state_space):\r\n",
    "        \r\n",
    "        self.action_space = action_space\r\n",
    "        self.state_space = state_space\r\n",
    "        self.epsilon = 1\r\n",
    "        self.gamma = .95\r\n",
    "        self.batch_size = 64\r\n",
    "        self.epsilon_min = .01\r\n",
    "        self.epsilon_decay = .995\r\n",
    "        self.learning_rate = 0.001\r\n",
    "        #memory that contain 10000 elemnts can pop and apend at end and start\r\n",
    "        self.memory = deque(maxlen=10000)\r\n",
    "        self.model = self.build_model()\r\n",
    "\r\n",
    "    def build_model(self):\r\n",
    "\r\n",
    "        model = Sequential()\r\n",
    "        model.add(Dense(24, input_dim=self.state_space, activation='relu'))\r\n",
    "        model.add(Dense(24, activation='relu'))  \r\n",
    "        model.add(Dense(self.state_space, activation='linear')) \r\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\r\n",
    "        return model\r\n",
    "\r\n",
    "    def remember(self, state, action, reward, next_state, done):\r\n",
    "        self.memory.append((state, action, reward, next_state, done))\r\n",
    "\r\n",
    "    def act(self, state):\r\n",
    "\r\n",
    "        if np.random.rand() <= self.epsilon:\r\n",
    "            return random.randrange(self.action_space)\r\n",
    "        act_values = self.model.predict(state)\r\n",
    "        return np.argmax(act_values[0])\r\n",
    "\r\n",
    "    def replay(self):\r\n",
    "\r\n",
    "\r\n",
    "# q_values[i][a] = r + self.gamma * np.max(nq_values[i])\r\n",
    "\r\n",
    "        if len(self.memory) < self.batch_size:\r\n",
    "            return\r\n",
    "\r\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\r\n",
    "        states = np.array([i[0] for i in minibatch])\r\n",
    "        actions = np.array([i[1] for i in minibatch])\r\n",
    "        rewards = np.array([i[2] for i in minibatch])\r\n",
    "        next_states = np.array([i[3] for i in minibatch])\r\n",
    "        dones = np.array([i[4] for i in minibatch])\r\n",
    "\r\n",
    "        #squeeze removes 1 dimentional\r\n",
    "        states = np.squeeze(states)\r\n",
    "        next_states = np.squeeze(next_states)\r\n",
    "\r\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))\r\n",
    "        targets_full = self.model.predict_on_batch(states)\r\n",
    "        print(targets.shape)\r\n",
    "        ind = np.array([i for i in range(self.batch_size)])\r\n",
    "        indValues=np.array([i for i in range(64)])\r\n",
    "        print(ind.shape)\r\n",
    "        print(actions.shape)\r\n",
    "        print(targets)\r\n",
    "        \r\n",
    "       \r\n",
    "        targets_full[[ind], [actions]] = targets\r\n",
    "       \r\n",
    "        \r\n",
    "        \r\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=1)\r\n",
    "        if self.epsilon > self.epsilon_min:\r\n",
    "            self.epsilon *= self.epsilon_decay\r\n",
    "\r\n",
    "\r\n",
    "def train_dqn(episode):\r\n",
    "\r\n",
    "    loss = []\r\n",
    "    agent = DQN(env.action_space.n, env.observation_space.shape[0])\r\n",
    "    for e in range(episode):\r\n",
    "        state = env.reset()\r\n",
    "        state = np.reshape(state, (1, -1))\r\n",
    "        score = 0\r\n",
    "        max_steps = 1000\r\n",
    "        for i in range(max_steps):\r\n",
    "            env.render()\r\n",
    "            action = agent.act(state)\r\n",
    "            next_state, reward, done, _ = env.step(action)\r\n",
    "            score += reward\r\n",
    "            next_state = np.reshape(next_state, (1, -1))\r\n",
    "            agent.remember(state, action, reward, next_state, done)\r\n",
    "            state = next_state\r\n",
    "            agent.replay()\r\n",
    "            if done:\r\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\r\n",
    "                break\r\n",
    "        loss.append(score)\r\n",
    "    return loss\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "\r\n",
    "    ep = 5\r\n",
    "    loss = train_dqn(ep)\r\n",
    "    plt.plot([i+1 for i in range(0, ep, 2)], loss[::2])\r\n",
    "    plt.show()\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "episode: 0/5, score: 32\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\programas\\anaco\\envs\\pgd\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "[1158.56225586 1158.56225586 1158.56225586 1158.56225586 1156.56225586\n",
      "   36.78439713 1158.56225586 1158.56225586 1158.56225586 1158.56225586\n",
      " 1158.56225586 1156.56225586 1158.56225586 1158.56225586 1158.56225586\n",
      " 1158.56225586 1158.56225586 1158.56225586 1158.56225586 1158.56225586\n",
      " 1158.56225586 1156.56225586   36.78439713 1158.56225586 1158.56225586\n",
      " 1158.56225586 1156.56225586 1158.56225586 1156.56225586 1158.56225586\n",
      " 1158.56225586 1158.56225586 1158.56225586   36.78439713 1158.56225586\n",
      " 1158.56225586 1158.56225586 1158.56225586 1156.56225586 1158.56225586\n",
      " 1158.56225586 1156.56225586 1158.56225586 1158.56225586   36.78439713\n",
      " 1158.56225586 1158.56225586 1158.56225586 1156.56225586 1156.56225586\n",
      " 1158.56225586 1156.56225586 1158.56225586 1158.56225586 1158.56225586\n",
      " 1158.56225586 1156.56225586 1158.56225586 1156.56225586 1156.56225586\n",
      " 1156.56225586 1158.56225586 1158.56225586 1158.56225586]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for axis 1 with size 10",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5356/3067558016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5356/3067558016.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[1;34m(episode)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"episode: {}/{}, score: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5356/3067558016.py\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mtargets_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 15 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pgd': conda)"
  },
  "interpreter": {
   "hash": "cabac2954cf654a7c1a0a2cb4143c0d9684498a99fac728f492c82ef13f7b48d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}